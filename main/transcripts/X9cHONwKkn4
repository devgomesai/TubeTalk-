[00:05] [Music] [00:50] This is how intelligence is made. [00:54] A new kind of factory, [00:57] generator of tokens, the building blocks [01:00] of AI. [01:03] tokens have opened a new frontier. [01:06] The first step into an extraordinary [01:08] world [01:10] where endless possibilities are born. [01:18] Tokens transform images into scientific [01:20] data, [01:23] charting alien atmospheres [01:25] and guiding the explorers of tomorrow. [01:32] They probe the earth's depths [01:35] to seek out hidden danger. [01:40] They turn potential into plenty [01:43] [Music] [01:47] and help us harvest our bounty. [01:53] Tokens see disease before it takes hold. [02:01] Cure with precision [02:05] [Music] [02:07] and learn what makes us tick. [02:15] Tokens connect the dots [02:18] so we can protect our most noble [02:20] creatures. [02:26] Tokens decode the laws of physics [02:31] to move us faster [02:38] and make our days more efficient. [02:45] Tokens don't just teach robots how to [02:48] move, but to bring joy. [02:53] and comfort. Hi, Maroka. Hi, Anna. Are [02:57] you ready to see the doctor? What's [03:00] that? It's my enchanted jewel. [03:03] [Music] [03:08] Tokens help us move forward. [03:13] One small step for man [03:18] becomes one giant leap for mankind. [03:22] So we can boldly go where no one has [03:26] gone before. [03:28] [Music] [03:38] And here [03:40] is where it all begins. [03:48] [Laughter] [03:49] [Music] [03:58] welcome to the stage Nvidia founder and [04:01] CEO Jensen Wong. [04:03] [Music] [04:11] Hello Paris. [04:23] Bonjour. [04:26] Nvidia's first GTC in Paris. [04:30] This is incredible. [04:34] Thank you for all the partners who are [04:36] here with us. [04:39] We have so many people that we work with [04:41] over the years. In fact, we've been in [04:43] Europe for a very long time. Even though [04:45] this is my first GTC Paris, I have a lot [04:47] to tell you. [04:50] Nvidia [04:52] once upon a time wanted to create a new [04:55] computing platform [04:58] to do things that normal computers [05:00] cannot. [05:02] We accelerated the CPU, created a new [05:06] type of computing called accelerated [05:07] computing. And one of our first [05:09] applications was molecular dynamics. [05:12] We've come a long way since. [05:16] So many different libraries. And in [05:18] fact, what makes accelerated computing [05:21] special is it's not just a new processor [05:26] that you compile software to. You have [05:29] to reformulate how you do computing. You [05:32] have to reformulate your algorithm. And [05:35] it turns out to be incredibly hard for [05:38] people to reformulate software and [05:40] algorithms to be highly paralyzed. [05:43] And so we created libraries to help each [05:47] market, each domain of application [05:50] become accelerated. Each one of these [05:52] libraries opens up new opportunities for [05:56] the developers and it opens up new [05:58] opportunities for growth for us and our [06:01] ecosystem partners. Computational [06:04] lithography the probably the single most [06:06] important applications in semiconductor [06:08] design today runs in a factory at TSMC [06:12] Samsung large semiconductor fabs. Before [06:17] the chip is made, it runs through an [06:19] inverse physics algorithm called coup [06:21] litho, computational lithography, [06:25] direct sparse solvers, [06:28] algebraic multi-grid solvers. Coop, we [06:32] just open sourced incredibly exciting [06:36] application library. This library [06:39] accelerates decision making to optimize [06:43] problems with millions of variables for [06:47] millions of constraints like traveling [06:49] salespeople problems. [06:51] Warp a pythonic [06:54] framework for expressing geometry and [06:57] physics solvers. really important QDF, [07:01] QML [07:03] structure databases, [07:05] data frames, classical machine learning [07:08] algorithms. QDF accelerates Spark, zero [07:13] lines of code change. QML accelerates [07:16] Scikiticle Learn, zero lines of code [07:19] change. Dynamo and QDNN. QDNN is [07:23] probably the single most important [07:25] library Nvidia has ever created. It [07:28] accelerates the primitives of deep [07:31] neural networks and Dynamo is our brand [07:34] new library that makes it possible to [07:36] dispatch, orchestrate, distribute [07:39] extremely complex inference workloads [07:42] across an entire AI factory. C [07:46] equivariance and coupensor tensor [07:48] contraction algorithms. [07:51] Equal variance is for neuronet networks [07:53] that obey the laws of geometry such as [07:56] proteins, molecules. Aerial and shiona [08:00] really important framework to enable AI [08:04] to run 6G [08:07] Earth 2, our simulation environment for [08:11] foundation models of weather and climate [08:14] models. Kilometer square incredibly high [08:18] resolution. Monai, our framework for [08:21] medical imaging, incredibly popular. [08:24] Parabicks, our solver for genomics [08:27] analysis, incredibly successful, coup [08:32] I'll talk about in just a second for [08:34] quantum computing and coupler [08:38] for numpy and scypi. As you could see, [08:42] these are just a few of the examples of [08:45] libraries. There are 400 others. Each [08:49] one of them accelerates a domain of [08:51] application. Each one of them opens up [08:53] new opportunities. [08:55] Well, one of the most exciting [08:58] most exciting is CUDA Q. CUDA X is this [09:03] suite of libraries, a library suite for [09:06] accelerating applications and [09:08] algorithms. On top of CUDA, we now have [09:11] CUDA Q. CUDA Q is for quantum computing [09:16] for classical quantum quantum classical [09:21] computing based on GPUs. [09:23] We've been working on CUDAQ now for [09:25] several years and today I can tell you [09:29] there's an inflection point happening in [09:31] quantum computing. As you know the first [09:35] physical cubit was demonstrated some [09:38] nearly 30 years ago. [09:40] An error correction algorithm was [09:42] invented in 1995 [09:44] and in 2023 almost 30 years later the [09:49] world's first logical cubit was [09:52] demonstrated by Google. Now since then a [09:55] couple of years later the number of [09:57] logical cubits which is represented by a [10:01] whole lot of physical cubits with error [10:03] correction the number of logical cubits [10:06] are starting to grow. Well, just like [10:08] Moore's law would, I could I could [10:11] totally expect 10 times more logical [10:14] cubits every 5 years, a 100 times more [10:16] logical cubits every 10 years. Those [10:19] logical cubits would become better error [10:22] corrected, more robust, higher [10:24] performance, more resilient, and of [10:26] course will continue to be scalable. [10:28] Quantum computing is reaching an [10:31] inflection point. We've been working [10:32] with quantum computing companies all [10:35] over the world in several different [10:37] ways, but here in Europe, there's a [10:39] large community. Uh I saw Pascal last [10:42] night. I saw Barcelona supercomputing [10:45] last night. It is clear now we are [10:48] within reach of being able to apply [10:50] quantum computing, quantum classical [10:53] computing in areas that can solve some [10:55] interesting problems in the coming [10:56] years. [10:59] This is a really exciting time. And so [11:01] we've been working with all of the [11:03] supercomputing centers. It's very clear [11:05] now that over the next several years or [11:08] at least the next generation of [11:09] supercomputers, every single one of them [11:11] will have a QPU assigned and QPU [11:14] connected to GPUs. The QPU will do [11:16] quantum computing of course and the GPUs [11:19] would be used for pre-processing for [11:21] control for error correction which would [11:23] be intensely computationally intensive [11:26] post-processing and such. between the [11:28] two architectures. Just as we [11:31] accelerated the CPU, now there's QPU [11:35] working with the GPU to enable the next [11:38] generation of computing. Well, we've [11:40] today we're announcing that our entire [11:45] quantum algorithm stack is now [11:47] accelerated on Grace Blackwell 200 and [11:51] the speed up is utterly incredible. We [11:54] work with the comput quantum computing [11:56] industry in several different ways. One [11:58] way is using coup quantum to simulate [12:02] the cubits or simulate the algorithms [12:05] that runs on top of uh these quantum [12:08] computers. Essentially using a classical [12:10] computer to simulate or emulate a [12:14] quantum computer. [12:16] At the other extreme, extremely [12:19] important is CUDA Q. Basically inventing [12:22] a new CUDA that extends CUDA into [12:25] quantum classical so that applications [12:28] that are developed on CUDA Q can run [12:31] before the quantum computer arrives in [12:34] an emulated way or after the quantum [12:37] computer arrives in a collaborative way, [12:41] a quantum classical accelerated [12:44] computing approach. And so today we're [12:46] announcing CUDAQ is available for Grace [12:50] Blackwell. The ecosystem here is [12:52] incredibly rich and of course Europe is [12:55] deep with science and deep with [12:57] supercomputing expertise and deep with [12:59] heritage in this area. And it's not [13:01] surprising to see quantum computing [13:04] advance here in the next several years. [13:06] We're going to see a really fantastic [13:09] inflection point. So anyways, for all of [13:11] the quantum computer industry that have [13:14] been working on this for three decades [13:15] now, I congratulate you for just the [13:18] incredible accomplishment and the [13:20] milestones today. Thank you. [13:26] [Applause] [13:30] Let's talk about AI. I you might be [13:33] surprised [13:35] that I would have I would be talking to [13:36] you about AI. The same the same GPU that [13:42] ran and enabled all of these [13:44] applications that I mentioned, that same [13:46] GPU enabled artificial intelligence to [13:51] come to the world. Our first contact was [13:54] in 2012, just prior to that, working [13:57] with developers on a new type of [13:59] algorithm called deep learning. It [14:01] enabled the AlexNet big bang of AI 2012. [14:07] In the last 15 years or so, AI has [14:11] progressed incredibly fast. The first [14:14] wave of AI was perception for computers [14:17] to recognize information, understand it. [14:21] The second wave, which most of us were [14:23] talking about the five the last five [14:25] years or so, was generative AI. It's [14:28] multimodal, meaning that an AI was able [14:31] to learn both images and language. [14:34] Therefore, you could prompt it with [14:36] language and it could generate images. [14:38] The ability for AI to be multimodal as [14:42] well as able to translate and generate [14:45] content enabled generative AI [14:47] revolution. Generative AI, the ability [14:50] to generate content is fundamentally [14:52] vital for us to be productive. [14:56] Well, we've got a new we are starting a [14:58] new wave of AI. And this last couple of [15:01] years, we've seen enormous progress in [15:04] AI's ability. Fundamentally, [15:07] intelligence is about understanding, [15:12] perception, [15:14] reasoning, [15:15] planning a task, how to solve a problem, [15:18] and then executing the task. Perception, [15:21] reasoning, planning. the fundamental [15:24] cycles of intelligence. It allows us to [15:27] apply some previously learned rules to [15:30] solve problems we've never seen before. [15:33] That's why [15:35] intelligent people are considered [15:36] intelligent. to be able to take a [15:38] complicated problem, break it down step [15:41] by step, [15:43] reason about how to solve the problem, [15:45] maybe do research, maybe go learn some [15:48] new information, get some help, use [15:50] tools, and solve problems step by step. [15:54] Well, the words that I just described [15:56] are fundamentally possible today with [15:58] what is called agentic AI. And I'll show [16:00] you more in just a second. In the [16:03] physical implementation of that, the [16:04] embodiment of that agentic AI [16:08] and the motion now the generative [16:10] capability is generating motion. Instead [16:13] of generating videos and generating [16:14] images or generating text, this AI [16:17] generates local motion. The ability to [16:20] walk or reach out and grab something, [16:22] use tools. The ability for AI to be [16:25] embodied in the physical form is [16:28] basically robotics. [16:30] These capabilities, the fundamental [16:32] technology to enable agents which are [16:35] basically information robots and [16:38] embodied AI, physical robots, these two [16:42] fundamental capabilities are now upon [16:45] us. Really, really exciting times for [16:47] AI. But it all started, it all started [16:51] with GeForce. [16:53] And GeForce brought computer graphics. [16:55] This is the first accelerated computing [16:57] application we had ever worked on and [17:00] it's incredible how far computer [17:02] graphics has come. GeForce brought CUDA [17:06] to the world which enabled m machine [17:09] learning researchers and AI researchers [17:12] to advance deep learning. [17:14] Then deep learning revolutionized [17:17] computer graphics and made it possible [17:19] for us to bring computer graphics to a [17:22] whole new level. Everything I'm going to [17:25] show you today, everything I'm going to [17:27] show you today, I'm going to give you a [17:28] preview of what I'm going to show you. [17:30] But everything I'm going to show you [17:31] today is computer simulation, not [17:35] animation. [17:37] It's photon simulation, physics [17:39] simulation, particle simulations. [17:42] Everything is fundamentally simulation, [17:45] not animation, not art. It just looks [17:48] incredibly beautiful because it turns [17:50] out the world is beautiful. And it turns [17:52] out math is beautiful. So let's take a [17:55] look. [17:57] [Music] [18:09] [Music] [18:13] Heat. Heat. [18:15] [Music] [18:31] [Music] [18:51] Oh, I've got a cramp coming on. Oh, I [18:53] can feel it. [19:07] [Music] [19:13] Heat. [19:16] Heat. [19:19] [Music] [19:25] Heat [19:28] [Music] [19:45] up here. [19:48] [Music] [20:04] What do you think? [20:13] Numbers in action. Numbers in action. [20:15] That's essentially what simulations are. [20:17] And it's just incredibly beautiful to [20:19] look at. But because of the scale [20:23] and the speed by which we can now [20:25] simulate almost everything, we can turn [20:28] everything into a digital twin. And [20:30] because everything can be a digital [20:32] twin, it could be designed, planned, [20:35] optimized, and operated completely [20:37] digitally before we put it into the [20:40] physical world. The idea that we would [20:42] build everything in software is now upon [20:45] us. Everything physical will be built [20:47] digitally. Everything that's built [20:49] magnificently will be built digitally. [20:51] Everything that's operated at gigantic [20:53] scale will be first built digitally and [20:56] there will be digital twins that operate [20:58] it. And so today we're going to talk a [20:59] lot about digital twins. Well, what [21:01] started out as a GeForce graphics card, [21:04] anybody in here know what a GeForce is? [21:08] Okay. All right. Well, what started out [21:12] as GeForce [21:13] looks like this now. This is the new [21:16] GeForce. It is two tons, two and a half [21:20] tons, 1.2 million parts, [21:25] about $3 million. [21:30] 120 kilowatts, [21:33] manufactured in 150 factories, [21:37] 200 technology partners working with us [21:39] to do this. probably something along the [21:42] lines of $40 billion in R&D budget in [21:46] order to create what is GB200 and now [21:51] moving to GB300. It is completely in [21:53] production. And this the machine was [21:56] designed to be a thinking machine. A [21:58] thinking machine in the sense that it [22:00] reasons, it plans, [22:03] it spends a lot of time talking to [22:04] itself [22:06] just like you do. [22:08] We spend most of our time generating [22:12] words for our own mind, generating [22:14] images for our own mind before we [22:16] produce it. And so the thinking machine [22:19] is really architecturally what Grace [22:21] Blackwell was designed to do. It was [22:23] designed to be one giant GPU. I compared [22:26] it to GeForce for a good reason. GeForce [22:29] is one GPU. So is GB200. It is one giant [22:35] virtual GPU. Now we had to disagregate [22:38] it into a whole bunch of components. [22:41] Create a a bunch of new networking [22:43] technology and sides technology [22:45] incredibly low low power high energy [22:48] efficiency interconnects to connect all [22:51] of these chips and systems together into [22:54] one virtual GPU. This is the Hopper [23:00] version. This is the world famous Hopper [23:02] system. [23:04] this eight GPUs connected together on [23:06] MVLink. What's not shown here is a CPU [23:10] tray. A CPU tray with dual CPU and [23:13] system memory that sits on top. [23:16] Together, this represents one node of an [23:20] AI supercomput [23:22] about half a million dollars. This is [23:25] the Hopper system. This is the the [23:27] system that really put us on the face of [23:29] face of the map of of uh AI. And uh it [23:33] was an it was under allocation for a [23:35] very long time uh because the the market [23:37] took off so quickly. But this is the [23:39] famous hopper system. Well, this entire [23:42] system including the CPU is replaced by [23:45] this great blackwell node. This is one [23:49] compute tray [23:52] right here. Will replace that entire [23:53] system. It is fully liquid cooled [23:57] and the CPUs are integrated directly [23:59] connected to the GPUs. So you could see [24:01] it here. Two CPUs, four GPUs is more [24:05] performant than that entire system. [24:08] But what's amazing is this. We wanted to [24:11] connect a whole bunch of these systems [24:13] together. How would you connect all of [24:14] these together was really hard for us to [24:17] imagine. So we disagregated it. What we [24:20] did was we took that entire motherboard. [24:23] We disagregated into this and this. This [24:27] is the revolutionary MVLink system. [24:31] Scaling out computing is not that hard. [24:34] Just connect more CPUs with Ethernet. [24:37] Scaling out is not hard. Scaling up is [24:40] incredibly hard. You can only build as [24:43] large of a computer as you can build. [24:46] The amount of technology and electronics [24:48] that you could fit into one memory [24:51] memory model is incredibly hard to do. [24:54] And so what we decided to do was we [24:56] created a new interconnect called [24:57] MVLink. MVLink is a memory semantics [25:01] interconnect. It's a compute fabric, not [25:04] a network. It directly connects to the [25:06] CPUs of all of these different MVLink [25:09] systems, compute nodes. This is the [25:12] switch. Nine of these, nine of these [25:18] stands on top. Nine of it stands sits on [25:21] the bottom. In the middle are the MVLink [25:24] switches. And what connects it together [25:26] is this miracle. [25:31] This is the MVLink spine. [25:34] This is 100% copper [25:37] copper coax. It directly connects all of [25:41] the MVLink chips to all of the GPUs [25:44] directly connected over this entire [25:47] spine so that every single [25:52] 144 Blackwell dies or in 72 different [25:56] packages are talking to each other at [25:59] the same time on without blocking all [26:02] across this MVLink spine. The bandwidth [26:05] of this is about 130 terabytes per [26:08] second. 132 I I know. [26:12] [Applause] [26:17] No, wait. Wait for it. Wait for it. [26:20] 130 terabytes per second. If it's in [26:23] bits [26:27] 130 terabytes per second, it is more [26:30] than the data rate of the peak traffic [26:33] of the world's entire internet traffic [26:37] on this back plane. And yeah, [26:44] so this is how this is how you shrink [26:46] the internet into 60 pounds. [26:50] MVLink. [26:51] And so we did all that. We did all that [26:53] because the the way a computer is is [26:56] considered, the way you think about [26:58] computers is going to be fundamentally [27:00] different in the future. I'll spend more [27:01] time on this, but it was designed to [27:04] give Blackwell a giant leap [27:07] above Hopper. Remember Moore's law, [27:10] semiconductor physics, is only giving [27:12] you about two times more performance [27:16] every three to five years. How could we [27:18] achieve 30 40 times more performance in [27:22] just one generation? And we need a 30 40 [27:25] times more performance because the [27:27] reasoning models are talking to [27:29] themselves. Instead of one shot chat [27:33] GPT, it's now a reasoning model and it [27:36] generates a ton more tokens when you're [27:38] thinking to yourself. You're breaking [27:40] the problem down step by step. You're [27:43] reasoning. You're trying a whole bunch [27:44] of different paths. Maybe it's chain of [27:46] thoughts. Maybe it's tree of thoughts [27:48] best of end. It's reflecting on its own [27:51] answers. You probably seen these [27:54] research models reflecting on the [27:56] answers saying is this a good answer? [27:58] Can you do better than that? And they, [28:00] oh yeah, I can do better than that. Goes [28:01] back and thinks some more. And so those [28:04] thinking models, reasoning models [28:07] achieved c incredible performance, but [28:09] it requires a lot more computational [28:12] capability. And what net result MVLink [28:16] 72 Blackwell's architecture [28:20] resulted in a giant leap in performance. [28:23] The way to read this is the X- axis is [28:26] how fast it's thinking. The Y axis is [28:30] how much the factory can output [28:32] supporting a whole bunch of users at one [28:34] time. And so you want the throughput of [28:37] the factory to be as high as possible. [28:39] So you could support as many people as [28:41] possible so that the revenues of your [28:43] factory is as high as possible. You want [28:46] this axis to be as large as possible [28:49] because the AI is smart smarter here [28:53] than it is here. The more the faster it [28:56] can think, the more it can think before [28:58] it answers your answer. And so this has [29:00] to do with the ASP of the the average [29:03] selling price of the tokens and this has [29:06] to be to do with the throughput of the [29:08] factories. These two combined in that [29:11] corner is the revenues of the factory. [29:14] This factory [29:16] based on Blackwell can generate a ton [29:19] more revenues as a result of the [29:21] architecture. It is such an incredible [29:24] thing what we built. We made a movie for [29:27] you just to give you a sense of the [29:29] enormity of the engineering that went [29:31] into building Grace Blackwell. Take a [29:33] look. [29:35] [Music] [29:38] Blackwell is an engineering marvel. [29:42] It begins as a blank silicon wafer. [29:45] [Music] [29:48] Hundreds of chip processing and [29:51] ultraviolet lithography steps build up [29:53] each of the 200 billion transistors [29:56] layer by layer on a 12in wafer. [30:00] The wafer is scribed into individual [30:02] Blackwell dye, tested and sorted, [30:05] separating the good dyes to move [30:07] forward. The chip on wafer on substrate [30:11] process attaches 32 Blackwell dyes and [30:14] 128 HPM stacks on a custom silicon [30:17] interposer wafer. [30:22] Metal interconnect traces are etched [30:24] directly into it, connecting Blackwell [30:27] GPUs and HBM stacks into each system and [30:30] package unit, locking everything into [30:33] place. Then the assembly is baked, [30:36] molded, and cured, creating the [30:38] Blackwell B200 Super Chip. Each [30:42] Blackwell is stress tested in ovens at [30:45] 125° [30:48] and pushed to its limits for several [30:50] hours. [30:54] Robots work around the clock to pick and [30:56] place over 10,000 components onto the [30:59] Grace Blackwell PCB. [31:04] Meanwhile, custom liquid cooling copper [31:06] blocks are prepared to keep the chips at [31:08] optimal temperatures. [31:17] At another facility, Connect X7 Super [31:20] Nix are built to enable scale out [31:22] communications and Bluefield 3 DPUs to [31:25] offload and accelerate networking, [31:28] storage, and security tasks. [31:31] All these parts converge to be carefully [31:34] integrated into GB200 compute trays. [31:40] [Music] [31:44] MVLink is the breakthrough high-speed [31:47] link that Nvidia invented to connect [31:49] multiple GPUs and scale up into a [31:52] massive virtual GPU. [31:55] The MVLink switch tray is constructed [31:57] with MVLink switch chips providing 14.4 [32:01] tab per second of all toall bandwidth. [32:05] MVLink spines form a custom blindmated [32:08] back plane with 5,000 copper cables [32:11] connecting all 72 black wells or 144 GPU [32:14] dies into one giant GPU delivering 130 [32:18] tab per second of all to all bandwidth [32:21] more than the global internet's peak [32:23] traffic from around the world. parts [32:26] arrive to be assembled by skilled [32:28] technicians into a rack scale AI [32:31] supercomput. [32:33] [Music] [32:42] In total, 1.2 2 million components, 2 [32:46] miles of copper cable, 130 trillion [32:49] transistors, weighing nearly 2 tons. [32:53] [Music] [32:54] Blackwell is more than a technological [32:56] wonder. It's a testament to the power of [32:59] global collaboration and innovation, [33:01] fueling the discoveries and solutions [33:03] that will shape our future everywhere. [33:07] We are driven to enable the geniuses of [33:10] our time to do their life's work. and we [33:13] can't wait to see the breakthroughs you [33:15] deliver. [33:25] Grace Blackwell Systems [33:28] all in production. It is really a [33:30] miracle. It's a miracle from a [33:32] technology perspective, but the supply [33:33] chain that came together to build these [33:36] GB200 systems, two tons each. We're [33:39] producing them now a thousand systems a [33:43] week. [33:44] No one has ever produced mass-produced [33:47] supercomputers at this scale before. [33:49] Each one of these racks is essentially [33:52] an entire supercomput. Only in 2018, the [33:57] largest Volta system, the Sierra [34:00] supercomputer in 2018 is less performant [34:03] than one of these racks. And that system [34:05] was 10 megawws. This is 100 kilowatts. [34:10] So the difference generationally between [34:13] 2018 and now, we've really taken [34:15] supercomputing, AI supercomputing to a [34:17] whole new level. And we're now producing [34:19] these machinery at enormous scales. And [34:22] this is just the beginning. [34:24] In fact, what you've seen is just one [34:28] system, Grace Blackwell. The entire [34:31] world is talking about this one system, [34:33] clamoring for it to get deployed into [34:35] into the world's data centers for [34:37] training and inferencing and generative [34:38] AI. However, not everybody and not every [34:43] data center can handle these liquid cool [34:45] systems. Some data centers required [34:48] enterprise stacks, the ability to run [34:50] Linux, Red Hat or Newonix or VMware [34:54] storage systems from Dell EMC, Hitachi, [34:58] NetApp, Vast, Weta, so many different [35:01] storage systems, so many different IT [35:04] systems and the management of those has [35:07] to be done in the way that's consistent [35:09] with traditional IT systems. We have so [35:11] many new computers to ramp into [35:14] production and I'm so happy to tell you [35:16] that every single one of these are now [35:19] in production. You haven't seen them [35:21] yet. They're all flying off the shelves, [35:24] flying off the ramps, the manufacturing [35:26] lines starting here. DGX Spark enables [35:30] you to have essentially the Grace [35:32] Blackwell system [35:34] on your desktop. In the case of Spark [35:37] Desktop, in the case of DJX station desk [35:39] side, this way you don't have to sit on [35:43] a supercomput while you're developing [35:45] your software, while you're developing [35:46] your AI, but you want the architecture [35:48] to be exactly the same. These systems [35:52] are identical from an architecture [35:55] perspective. From a software developer [35:57] perspective, it looks exactly the same. [36:00] The only difference is scale and speed. [36:02] And then on this side are all the x86 [36:05] systems. The world's IT organization [36:08] still prefers x86 and appreciates x86. [36:11] Wherever they can take advantage of the [36:13] most advanced AI native systems, they [36:15] do. Where they can't and they want to [36:17] integrate into the enterprise IT [36:20] systems, we now offer them the ability [36:21] to do so. One of the most important [36:24] systems and it's taken us the longest to [36:27] build because the software and the [36:29] architecture is so complicated is how to [36:31] bring the AI native architecture and [36:34] infuse it into the traditional [36:37] enterprise IT system. This is our brand [36:40] new RTX Pro server. This is an [36:43] incredible system. [36:45] The motherboard is completely [36:47] redesigned. [36:50] Ladies and gentlemen, Janine Paul [36:53] [Applause] [36:59] This motherboard looks so simple. And [37:02] yet on top of this motherboard are eight [37:06] Super Nix switches that connect eight [37:08] GPUs across a 200 Gbits per second [37:12] state-of-the-art networking chip that [37:15] then connects eight of these GPUs. And [37:18] these Blackwell RTX Pro 6000 GPUs, brand [37:22] new, just entered into production. Eight [37:25] of these go into a server. Now, what [37:27] makes it special? [37:29] This server is the only server in the [37:31] world that runs everything the world has [37:34] ever written and everything Nvidia has [37:37] ever developed. It runs AI, [37:41] Omniverse, [37:44] RTX for video games. It runs Windows. It [37:47] runs Linux. It runs Kubernetes. It runs [37:50] Kubernetes in VMware. [37:52] It runs basically everything. If you [37:55] want to stream Windows desktop from a [37:57] computer to your to your remote device, [38:00] no problem. If you want to stream [38:01] Omniverse, no problem. If you want to [38:03] run your robotic stack, no problem. Just [38:06] a QA of this particular machine is [38:09] insane. The applications that it runs [38:12] basically universal. Everything the [38:14] world's ever developed should run on [38:16] here. including if you're a video gamer, [38:20] including Crisis. And so [38:28] if if you can run Crisis, you can run [38:30] anything. [38:32] Okay, this is the RTX Pro server, brand [38:35] new enterprise system. [38:40] So something is changing. [38:43] We know that AI is incredibly important [38:45] technology. [38:47] We know for a fact now that AI is [38:50] software that could revolutionize, [38:52] transform every industry. It can do [38:55] these amazing things. That we know. We [38:59] also know that the way you process AI is [39:02] fundamentally different than the way we [39:04] used to process software written by [39:06] hand. Machine learning software is [39:09] developed differently and it runs [39:11] differently. [39:12] The architecture of the systems, the [39:14] architecture of the software completely [39:16] different. The way the networking works [39:18] completely different. The way it acts as [39:20] storage completely different. [39:23] So we know that the technology can do [39:26] different things, incredible things. [39:29] It's intelligent. We also know that it's [39:32] developed in a fundamentally different [39:33] way. Needs new computers. [39:36] The thing that's really interesting is [39:38] what does this all mean to countries, to [39:42] companies, to society and this is this [39:45] is an observation that we made almost a [39:47] decade ago that now everyone is [39:49] awakening to that in fact these AI data [39:53] centers are not data centers at all. [39:55] They're not data centers in the [39:57] classical sense of a data center storing [40:00] your files that you retrieve. These data [40:03] centers are not storing our files. It [40:06] has one job and one job only to produce [40:10] intelligent tokens. The generation of [40:13] AI. These factories of AI are look like [40:18] data centers in the sense that they have [40:21] a lot of computers inside. But that's [40:23] where everything breaks down. how it's [40:26] designed, the scale at which it's [40:29] manufactured or scaled, designed and [40:32] built, and how it's used and how it's [40:36] orchestrated and provisioned, operated, [40:39] how you think about it. For example, [40:42] nobody really thinks about their data [40:44] center as a revenue generating facility. [40:50] I said something that everybody goes, [40:53] "Yeah, I think you're right. [40:55] Nobody ever thinks about a data center [40:57] as a revenue generating facility. But [40:59] they think of their factories, their car [41:01] factories as revenue generating [41:04] facilities and they can't wait to build [41:06] another factory because whenever you [41:08] build a factory, revenue grows shortly [41:11] after. You could build more things for [41:13] more people. Those ideas [41:17] are exactly the same ideas in these AI [41:20] factories. They are revenue generating [41:22] facilities and they are designed to [41:25] manufacture tokens and these tokens [41:28] can be reformulated into productive [41:31] intelligence for so many industries that [41:35] AI factories are now part of a country's [41:38] infrastructure which is the reason why [41:41] you see me running around the world [41:43] talking to heads of states because they [41:45] all want [41:47] to have AI factories. They all want AI [41:50] to be part of their infrastructure. They [41:52] want AI to be a growth manufacturing [41:55] industry for them. And this is [41:58] genuinely profound. And I think we're [42:01] talking about as a result of all that a [42:04] new industrial revolution because every [42:06] single industry is affected and a new [42:09] industry just as [42:13] electricity became a new industry. At [42:16] first when it was described as a [42:18] technology and demonstrated as a [42:19] technology, it was understood as a [42:22] technology but then we understood that [42:24] it's also a large industry. Then there's [42:26] the information industry which we now [42:28] know as the internet and both of them [42:31] because it affected so many industries [42:33] became part of infrastructure. We now [42:35] have a new industry an AI industry and [42:38] it's now part of the new infrastructure [42:41] called intelligence infrastructure. [42:43] Every country, every society, every c [42:45] company will depend on it. And you could [42:47] see its scale. This is one that's being [42:50] talked about a lot. This is Stargate. [42:52] This doesn't look like a data center. It [42:54] looks like a factory. This is 1 gawatt. [42:57] It will hold about 500,000 GPU dies and [43:01] produce an enormous amount of [43:03] intelligence that could be used by [43:04] everybody. Well, [43:08] Europe has now awakened to the [43:11] importance of these AI factories, the [43:14] importance of the AI infrastructure, and [43:16] I'm so delighted to see so much activity [43:18] here. This is um European Telos [43:22] AI infrastructure with Nvidia. [43:26] This is the European cloud service [43:28] providers building AI infrastructure [43:30] with NVIDIA. And this is the European [43:33] supercomputing centers building next [43:36] generation AI supercomputers and [43:37] infrastructure with NVIDIA. And this is [43:40] just the beginning. This is in addition [43:43] to what will come in the public clouds. [43:46] This is in addition to the public [43:48] clouds. So indigenous built AI [43:52] infrastructure here in Europe by [43:54] European companies for the European [43:56] market. And then there's 20 more being [44:00] planned. [44:01] 20 more AI factories and several that [44:04] are gigawatt gigafactories. [44:07] In total, [44:10] in just two years, we will increase the [44:14] amount of AI computing capacity in [44:16] Europe by a factor of 10. And so the [44:20] researchers, the startups, your AI [44:23] shortage, your GPU shortage will be [44:26] resolved for you soon. It's coming for [44:28] you. [44:37] Now, we're partnering with [44:40] each country to develop their ecosystem. [44:42] And so, we're building AI technology [44:45] centers in seven seven different [44:47] countries. And the goal of these AI [44:50] technology centers is one to do [44:53] collaborative research to work with the [44:55] startups and also to build the [44:57] ecosystem. Let me show you what an [44:58] ecosystem looks like in the UK. I was [45:01] just there yesterday. [45:03] The ecosystems are built on top of the [45:05] NVIDIA stack. So for example, every [45:09] single NVIDIA, as you know, NVIDIA is [45:11] the only AI architecture that's [45:12] available in every cloud. It's the only [45:15] computing architecture aside from x86 [45:18] that's available everywhere. We're avail [45:21] partner with every cloud service [45:22] provider. We accelerate applications [45:26] from the most important software [45:27] developers in the world. Seammens here [45:30] in Europe, Cadence, Red Hat, Service [45:32] Now. We've reinvented the computing [45:35] stack. As you know, computing is not [45:37] just a computer, but it's compute, [45:39] networking, and storage. Each one of [45:41] those layers, each one of those stacks [45:43] has been reinvented. Great partnership [45:45] with Cisco who announced a brand new [45:48] model yesterday at their conference [45:49] based on Nvidia, Dell, great [45:52] partnerships, NetApp, Newtonics, whole [45:54] bunch of great partnerships. [45:57] As I mentioned earlier, the way you [45:58] develop software has been fundamentally [46:01] changed. It's no longer just write C [46:04] program, compile C program, de deliver C [46:07] program. It's now DevOps, MLOps, AI ops. [46:13] So that entire ecosystem is being [46:15] reinvented and we have ecosystem [46:17] partners everywhere. And then of course [46:20] solution integrators and providers who [46:22] could then help every company integrate [46:24] these capabilities. Well, here in the [46:27] UK, we have special companies that we [46:29] work with, really terrific companies [46:31] from researchers to developers [46:34] to partners to help us upskill the local [46:39] economy and upskill the local talent, [46:41] enterprises that consume the technology, [46:43] and of course, cloud service providers. [46:45] We have great partners in the UK. We [46:48] have great partners in Germany, [46:50] incredible, incredible partnerships in [46:52] Germany. We have great partnerships in [46:55] Italy and we of course have amazing [46:58] partnerships here in France. [47:03] [Applause] [47:10] That's right. Go France. [47:18] [Applause] [47:26] President Mcron's going to be here later [47:27] on. We're going to talk about some some [47:29] new some new announcements. So, we have [47:32] to show some enthusiasm for AI. Okay. [47:38] Yeah. [47:40] There you go. Show him some enthusiasm. [47:45] So great partnerships here here here in [47:47] France. Uh one particular one I want to [47:49] highlight our partnership with Schneider [47:51] building the even building these AI [47:54] factories. We build them digitally now. [47:57] We design them digitally. We build them [48:00] digitally. We operate them or optimize [48:02] them digitally and we will even [48:04] eventually optimize them and operate [48:06] them completely digitally in a digital [48:08] twin. These AI factories are so [48:12] expensive, [48:13] $50 billion sometimes, hundred billion [48:16] dollars in the future. If the [48:18] utilization of that factory is not at [48:21] its fullest, the cost to the factory [48:24] owner is going to be incredible. And so [48:27] we need to digitalize and use AI [48:29] wherever we can put everything into [48:31] Omniverse so that we have direct and [48:34] constant telemetry. We have a great [48:36] partnership here that we're announcing [48:38] today. A young company, a CEO I really [48:42] like and he's trying to build a European [48:47] AI company. The name of the company is [48:50] Mistral. Today, [48:59] today we're announcing that we're going [49:01] to build an AI cloud together here to [49:04] deliver their models as well as deliver [49:06] AI applications for the ecosystem of a [49:09] other AI startups so that they can use [49:11] the MRO models or any model that they [49:13] like. And so, Mrol and we're going to be [49:16] partnering to build a very sizable AI [49:18] cloud here. And we'll tell we'll talk [49:20] about more of it later on today with [49:22] President Mcronone. [49:32] AI technology is moving at light speed. [49:35] And what I'm showing you here, [49:37] proprietary models on the left moving at [49:39] light speed. However, the open models [49:42] are also moving at light speed. Only a [49:45] few months behind. [49:48] Whether it's Mistral, Llama, Deep Seek, [49:52] R1, R2, coming Q1, these models are all [49:57] exceptional. Every single one of them [49:59] exceptional. And so we've dedicated [50:01] ourselves over the last several years to [50:03] apply some of the world's best AI [50:05] researchers to make those AI models even [50:10] better. And we call that Neotron. [50:13] Basically what we do is we take the [50:15] models that are open sourced and of [50:18] course they're all built on NVIDIA [50:19] anyhow and so we take those models open [50:22] sourced we then post train it we might [50:25] do neural architecture search [50:29] we might do neuroarchchitecture search [50:31] provide it with even better data use [50:34] reinforcement learning techniques [50:36] enhance those models give it reasoning [50:38] capabilities extend the context so that [50:42] it could learn and read more before it [50:44] interacts with you. Most of these models [50:48] have relatively short context and we [50:50] want it to have enormous context [50:52] capability because we want to use it in [50:54] enterprise applications where the [50:56] conversation we want to have with it is [50:57] not available on the internet. It's [50:59] available in our company and so we have [51:01] to load it up with enormous amount of [51:03] context. All of that capability is then [51:06] packaged together into a downloadable [51:08] NIM. You could come to Nvidia's website [51:11] and literally download an API, a [51:14] state-of-the-art AI model, [51:16] put it anywhere you like, and we improve [51:20] it tremendously. This is an example of [51:23] Neimotron improvement over llama. So, [51:26] Llama 8B, 70B, 405B improved by our [51:32] post-training capability, extension of [51:34] reason, reasoning capability, all the [51:37] data that we provide, enhanced it [51:39] tremendously. We're going to do this [51:41] generation after generation after [51:43] generation. And so, for all of you who [51:45] would uses Neotron, you will know that [51:48] there's a whole slew of other models in [51:50] the future. And they're open anyway. So, [51:52] if you would like to start from the open [51:54] model, that's terrific. If you'd like to [51:55] start with the Neotron model, that's [51:57] terrific. And the Neotron models, the [52:00] performance is excellent in benchmarks [52:02] after benchmarks after benchmarks. [52:05] Neotron performance has top of the [52:08] leaderboard all over the place. And so [52:10] now you know that you have access to a [52:13] enhanced open model that is still open, [52:16] that is top of the leader chart. And you [52:19] know that Nvidia is dedicated to this. [52:20] And so I will do this for as long as I [52:22] shall live. Okay, [52:30] this strategy is so good. This strategy [52:33] is so good that the regional model [52:36] makers, the model builders across Europe [52:39] have now recognized how wonderful the [52:41] strategy is. And we're partnering [52:42] together to adapt, enhance each one of [52:45] those models for regional languages. [52:49] Your data belongs to you. Your data [52:53] belongs to you. It is the history of [52:55] your people, the knowledge of your [52:57] people, the culture of your people. It [52:59] belongs to you. And for many companies, [53:02] in the case of Nvidia, our data is [53:04] largely inside. 33 years of data. I was [53:08] looking up this morning, Seammens, 180 [53:12] years of data, some of it written down [53:15] on papyrus. [53:20] Roland Bush is here. I I thought I'd [53:22] pick on Roland Bush, my good friend. And [53:24] so you'll have to digitize that before [53:27] the AI can learn. And so you the data [53:30] belongs to you. You should use that [53:32] data, use an open model like Limotron [53:35] and all the tool suites that we provide [53:37] so that you can enhance it for your own [53:39] use. We're also announcing that we have [53:42] a great partnership with Perplexity. [53:44] Perplexity is a reasoning search engine. [53:47] Yep. [53:53] The three models I use, I use Chat GPT, [53:56] Gemini, Pro, and Perplexity. And these [53:58] three models I use interchangeably. And [54:00] Perplexity is fantastic. We're [54:02] announcing today that Perplexity will [54:04] take these regional models and connect [54:07] it right into Perplexity so that you [54:09] could now ask and get questions in the [54:13] language, in the culture, in the [54:16] sensibility of your country. Okay. So [54:19] perplexity regional models [54:26] agentic AI [54:29] agentic AI agents is a very big deal as [54:33] you know in the beginning with [54:35] pre-trained models people said but it [54:37] hallucinates [54:39] it makes things up you're absolutely [54:42] right it doesn't have access to the [54:45] latest news and data information [54:48] Absolutely right. [54:50] It gives up without reasoning through [54:53] problems. It's as if every single answer [54:56] has to be memorized from the past. [54:58] You're absolutely right. All of those [55:01] things, you know, why is it trying to [55:03] figure out how to add or count the count [55:06] numbers and add numbers? Why doesn't it [55:08] use a calculator? You're absolutely [55:10] right. [55:12] And so all of those capabilities [55:14] associated with intelligence, everybody [55:16] was able to criticize, but they was [55:18] absolutely right because everybody [55:20] largely understand how intelligence [55:22] works. But those technologies were being [55:25] built all around the world and they were [55:28] all coming together from retrieval [55:31] augmented generation to web search to [55:35] multimodal understanding so that you can [55:38] read PDFs go to a website look at the [55:41] images and the words listen to the [55:43] videos [55:45] watch the videos [55:47] and then take all of that understanding [55:49] into your context. You could also now [55:52] understand of course a prompt from [55:54] almost anything. You could even say I'm [55:56] going to ask you a question but start [55:58] from this image. I could say start from [56:01] this start from this text before you [56:04] answer the question or do what I ask you [56:06] to do. It then goes off and reasons and [56:09] plans and evaluates itself. All of those [56:13] capabilities are now integrated and you [56:15] can see it coming out into the [56:16] marketplace all over the place. Agentic [56:19] AI is real. Agentic AI is a giant step [56:22] function from oneshot AI. The oneshot AI [56:27] was necessary to lay the foundation so [56:29] that we can teach the agents how to be [56:31] agents. You need some basic [56:34] understanding of knowledge and basic [56:36] understanding of reasoning to even be [56:38] able to be teachable. And so [56:40] pre-training is about teachability of [56:43] AI, post-training, reinforcement [56:46] learning, supervised learning, human [56:49] demonstration, [56:52] context provision, [56:55] generative AI. All of that is coming [56:57] together to formulate what is now a [57:00] gentic AI. Let's take a look at one [57:02] example. Let me show you something. It's [57:04] built on perplexity and it's super cool. [57:10] AI agents are digital assistants [57:14] based on a prompt. They reason through [57:16] and break down problems into multi-step [57:18] plans. They use the proper tools, work [57:22] with other agents, and use context from [57:25] memory to properly execute the job on [57:28] NVIDIA accelerated systems. [57:31] It starts with a simple prompt. Let's [57:33] ask Perplexity to help start a food [57:36] truck in Paris. First, the Perplexity [57:39] agent reasons through the prompt and [57:42] forms a plan. [57:44] Then calls other agents to help tackle [57:47] each step using many tools. The market [57:51] researcher reads reviews and reports to [57:54] uncover trends and analyze the [57:56] competitive market. [57:59] Based on this research, a concept [58:01] designer explores local ingredients and [58:04] proposes a menu complete with prep time [58:07] estimates [58:09] and researches pallets and generates a [58:12] brand identity. [58:14] Then the financial planner uses Monte [58:18] Carlos simulations to forecast [58:20] profitability and growth trajectory. [58:23] An operations planner builds a launch [58:26] timeline with every detail. From buying [58:28] equipment to acquiring the right [58:31] permits, [58:32] the marketing specialist builds a launch [58:35] plan with a social media campaign and [58:38] even codes an interactive website, [58:41] including a map, menu, and online [58:43] ordering. [58:45] Each agent's work comes together in a [58:48] final package proposal. And it all [58:51] started from a single prompt. [58:56] [Applause] [59:02] One prompt, one prompt like that on the [59:05] in the original chatbot would have [59:07] generated a few hundred tokens. But now [59:10] with that one single prompt into an [59:13] agent to solve a problem, it must have [59:16] generated 10,000 times more tokens. This [59:19] is the reason why Grace Blackwell is [59:21] necessary. This is the reason why we [59:23] need performance and the systems to be [59:26] so much more performant generationally. [59:28] Well, this is how Perplexity builds [59:31] their agents. Every company will have to [59:34] build their own agents. It's terrific. [59:36] You're going to be hiring agents from [59:38] OpenAI and Gemini and Microsoft Copilot [59:43] and Perplexity and Mistrol. There'll be [59:47] agents that are built for you and they [59:50] might help you plan a vacation or, you [59:53] know, go do some research so on so [59:56] forth. However, if you want to build a [59:58] company, you're going to need [60:00] specialized agents on specialized tools [60:02] and using specialized tools and [60:05] specialized skills. And so, the question [60:07] is, how do you build those agents? And [60:09] so, we created a platform for you. We [60:11] created a framework and a set of tools [60:14] that you can use and a whole bunch of [60:15] partners to help you do it. It starts [60:17] with on the very bottom on the very [60:19] bottom the ability to have reasoning [60:21] models that I spoke about. Nvidia's Nemo [60:25] Neotron reasoning large language models [60:27] are worldclass. We have Nemo Retriever [60:30] which is a multimodal search engine [60:33] semantic search engine. Incredible. And [60:35] we built a blueprint, a demonstration [60:39] that is operational that is essentially [60:41] a general agent. We call it IQ, AI, AIQ. [60:46] And on top we have a suite of tools that [60:50] allows you to onboard an agent, a [60:53] general agent, curate data to teach it, [60:57] evaluate it, guard rail it, supervise, [61:00] train it, use reinforcement learning all [61:04] the way to deployment, keep it secure, [61:07] keep it safe. [61:09] That suite of toolkits is integrated, [61:11] those libraries are integrated into the [61:13] AI ops ecosystem. You can come and [61:16] download it from our website yourself as [61:18] well. But it's largely integrated into [61:20] AI ops ecosystem from that. You could [61:23] create your own special agents. [61:26] Many companies are doing this. This is [61:28] Cisco. They announced it yesterday. [61:31] We're building AI platforms together for [61:34] security. Now look at this. [61:38] AI agents is not one model that does all [61:41] of these amazing things. It's a [61:43] collection, a system of models. It's a [61:45] system of AI large language models. Some [61:48] of them are optimized for certain type [61:50] of things. Retrieval as I mentioned, [61:52] performing skills using a computer. You [61:55] don't want to bundle all of that stuff [61:57] up into one giant, you know, mass of AI, [62:01] but you break it up into small things [62:03] that you could then deploy CI/CD over [62:05] time. This is an example of Cisco's. [62:08] Now, the question is how do you now [62:10] deploy this? Because as I mentioned [62:12] earlier there are public clouds where [62:14] Nvidia's compute is there are regional [62:18] clouds we call them NCPs here for [62:20] example mistrol you might have something [62:23] that is private cloud because of your [62:27] security requirements and your data data [62:29] privacy requirements you might even [62:31] decide that you have something on your [62:32] desk and so the question is how do you [62:35] run all of these and sometimes they will [62:37] run in different places because these [62:39] are all microservices these are AIs that [62:41] could talk to each other. They could [62:42] obviously talk to each other over [62:43] networking. And so, how do you deploy [62:46] all of these microservices? Well, we now [62:49] have a great system. I'm so happy to [62:51] announce this for you. This is called [62:53] our DGX Leptton. DGX Leptton. What [62:57] you're looking at here is a whole bunch [62:59] of different clouds. Here's the Lambda [63:01] cloud, the AWS cloud, you know, uh [63:04] here's your own developers machine, your [63:06] own system. Could be a DGX station, NBS, [63:09] Yoda, and Scale. It could be AWS. It [63:12] could be GCP. Nvidia's architecture is [63:14] everywhere. And so you could decide [63:17] where you would like to run your models. [63:19] You deploy it using one super cloud. So [63:23] it's a cloud of clouds. Once you get it [63:25] to work, once you get this NIMS deployed [63:28] into Lepton, it will go [63:32] and be hosted and run on the various [63:36] clouds that you decide. one model [63:39] architecture, one deployment, and you [63:42] can run it everywhere. You can even run [63:44] it on this little tiny machine here. [63:46] You know, this [63:49] this [63:50] DGX Spark, [63:53] it's [63:55] it's [63:59] Is this is it a cafe time? [64:05] Look at this. [64:08] [Applause] [64:14] This lift is 200 horsepower. [64:18] This is my favorite little machine, DGX [64:22] Spark. The first the AI supercomput. We [64:26] built an AI supercomput in 2016. It's [64:28] called the DGX1. It was the first [64:31] version of everything that I've been [64:32] talking about. eight Volta GPUs [64:36] connected with MVLink. [64:38] It took us billions of dollars to build [64:42] and on the day we announced it DGX1, [64:45] there were no customers, no interest, no [64:48] applause, [64:50] 100% confusion. [64:52] Why would somebody build a computer like [64:54] that? Does it run Windows? Nope. [65:01] And so we built it anyways. Well, thank [65:03] thankfully uh a young company, a [65:06] startup, a nonprofit startup in San [65:09] Francisco was so delighted to see the [65:11] computer. They said, "Can we have one?" [65:13] And I thought, "Oh my gosh, we sold [65:15] one." But then I discovered it was a [65:17] nonprofit. [65:20] But I put a computer, put a DGX1 in my [65:23] car and I drove it up to San Francisco [65:26] and the name of that company is Open AI. [65:37] I don't know what the life lesson is [65:38] there. There are a lot of nonprofits, [65:42] you know. So, next time next time, but [65:45] the maybe the lesson is this. If a [65:46] developer reaches out to you need a need [65:48] a GPU, the answer is yes. And so, so [65:52] that's right. [65:57] So, imagine you have leptton. It's in [65:59] it's in your browser and you have you [66:01] have uh this this uh Helm chart an AI [66:04] agent that you've developed and you want [66:06] to run it here and parts of it you want [66:08] to run in AWS and parts of it you want [66:10] to run you know in a regional cloud [66:12] somewhere you use leptton you deploy [66:14] your Helmchart and it magically shows up [66:17] here okay and so if you would like to [66:19] run it here until you're done with and [66:21] ready to deploy it then deploy it into [66:23] the cloud terrific but the beautiful [66:25] thing is this architecture is based on [66:27] Grace Blackwell Well, GB10 versus GB200 [66:32] versus GB2 300 and all of these [66:35] different versions of but this [66:36] architecture is exactly Grace Blackwell. [66:39] Now, this is amazing. So, we're doing [66:41] this for Lepon, but [66:45] next hugging face [66:48] and Nvidia has connected Leptton [66:50] together. And so whenever you're [66:53] training a model on hugging face, if you [66:54] would like to deploy it into leptton and [66:56] directly into Spark, no problem. It's [67:00] just one click. So whether you're [67:02] training or inferencing, we're now [67:05] connected to hugging face and leptton [67:08] will help you decide where you want to [67:10] deploy it. Let's take a look at it. [67:15] Developers need easy and reliable access [67:17] to compute that keeps up with their work [67:20] wherever they are, whatever they're [67:22] building. DJX Cloud Leptin provides [67:26] ondemand access to a global network of [67:28] GPUs across clouds, regions, and [67:32] partners like Yoda and Nebus. [67:36] Multicloud GPU clusters are managed [67:38] through a single unified interface. [67:43] Provisioning is fast. Developers can [67:46] scale up the number of nodes quickly [67:48] without complex setups and start [67:50] training right away with pre-integrated [67:52] tools and training ready infrastructure. [67:56] Progress is monitored in real time. GPU [67:58] performance, convergence, and throughput [68:01] are at your fingertips. [68:04] You can test your fine-tuned models [68:06] right within the console. DJX Cloud [68:08] Leptin can deploy NIM endpoints or your [68:11] models in multiple clouds or regions for [68:14] fast distributed inference. [68:18] Just like ride sharing apps connect [68:19] riders to drivers, DJX Cloud Leptin [68:22] connects developers to GPU compute, [68:25] powering a virtual global AI factory. [68:31] DGX cloud leton. [68:36] Okay, so that's Cisco. This is the way [68:38] SAP they're building an AI platform in [68:40] Nvidia. SA is building an AI business [68:44] application automation on NVIDIA. Deepl [68:48] is building their uh language framework [68:51] and platform on NVIDIA AI. Photo Room a [68:55] video editing uh and AI editing uh [68:58] platform building their platform on [69:00] NVIDIA. And this is Kodo used to be I [69:03] think Kodium incredible coding agent [69:06] built on Nvidia. And this is Iola a [69:09] voice platform built on Nvidia. And this [69:11] one is a uh a clinical trial platform [69:15] the world's largest uh automation [69:18] platform for clinical trials built on [69:20] Nvidia. And so all of these all of these [69:23] basically builds on the same idea. NIMS [69:28] that encapsulates and packages up in a [69:31] virtual container that you could deploy [69:33] anywhere the Neotron large language [69:36] model or other large language models [69:38] like Mistral or others. Uh we then [69:41] integrate libraries that basically [69:44] covers the entire life cycle of an AI an [69:48] AI agent. The way you treat an AI agents [69:51] a little bit like a digital employee. So [69:52] your IT department would have to onboard [69:55] them, fine-tune them, train them, [69:57] evaluate them, keep them guard railed, [70:00] you know, keep them secure and [70:02] continuously improve them. And that [70:04] entire framework platform is called [70:06] Nemo. And all of that is now being [70:08] integrated into one application [70:10] framework after another all over the [70:12] world. This is just an example of a few [70:14] of them. And then now we make it [70:16] possible for you to deploy them [70:17] anywhere. If you want to deploy it in [70:19] the cloud, you got DGx uh you got [70:21] GB200's in the cloud. If you want to [70:23] deploy it on prem because you've got uh [70:27] uh u VMware or Red Hat Linux or uh [70:30] Newonix and you want to deploy it in [70:32] your virtual machines on prem, you can [70:34] do that. If you want to deploy it as a [70:36] private cloud, you could do that. You [70:38] can deploy it all the way on your DGX [70:39] Spark or DGX station, no problem. And so [70:43] Lepton will help you do all of that. [70:46] Let's talk about industrial AI. This is [70:50] one of my favorite moments. [70:54] This is Roland Bush. He just This is a [70:57] really fun moment. He wanted to remind [70:59] me that neurocomputers, [71:03] neuronet network computers were invented [71:06] in Europe. [71:08] That's this whole slide. [71:10] Look, I just it is it was such a great [71:13] moment. This is the Synapse One. This is [71:17] incredible, you guys. Synapse one. This [71:21] is Synapse One 1992. It runs neural [71:25] networks 8,000 times faster than CPUs of [71:28] that time. Isn't it incredible? So, this [71:31] is the world's AI computer. [71:37] And and Roland just wants to just makes [71:40] forget that, Jensen. Never ever forget [71:43] that. I said, "Okay, all right. Good. [71:44] All right, I'll tell and I'll even tell [71:46] everybody Seammen's 1992. [71:50] Seammen's 1992. We have a great [71:52] partnership with Seaman Seammens and and [71:54] uh Roland Bush uh the CEO is uh [71:57] supercharging the company so that they [72:00] could leap [72:02] completely leap the last IT industrial [72:06] revolution and fuse the industrial [72:09] capabilities of Europe, the industrial [72:11] capabilities of might of Seammens with [72:13] artificial intelligence and create what [72:15] is called the industrial AI revolution. [72:19] We're partnering with Seammens on so [72:20] many different fronts. Uh everything [72:22] from design to simulation [72:27] to digital twins of factories to [72:30] operations of the AIs in the factories [72:34] everything from end to end. And it just [72:36] reminds us it reminds me uh how how [72:40] incredible the industrial capabilities [72:44] of Europe is and what an extraordinary [72:46] opportunity this is for you. What an [72:49] extraordinary opportunity because AI is [72:51] unlike software. AI is really really [72:55] smart software and this smart software [72:58] can finally do something that can [73:01] revolutionize the very industries that [73:03] you serve. And so we made made a a love [73:06] letter video if you will. Let's play it. [73:12] It began here. [73:16] The first industrial revolution, [73:19] Watt's steam engine and the mechanized [73:22] loom introduced automation [73:26] and the advent of factories [73:28] and industry was born. [73:32] The age of electricity. [73:36] Ampier unraveled electromagnetism. [73:39] [Music] [73:40] Faraday built the first electric [73:43] generator. and Maxwell laid the [73:45] foundations for modern electrical [73:48] engineering. [73:50] Seamans and Wheatstone dynamo, [73:54] the engine of electricity, [73:58] bringing machines, [74:00] trains, factories, and cities to life, [74:04] electrifying the planet, [74:08] igniting modern manufacturing. [74:14] And today, born out of the computing and [74:17] information age, the fourth industrial [74:20] revolution, [74:22] the age of AI, [74:24] reimagining every part of industry [74:28] across the continent, industrial AI is [74:31] taking hold. From design to engineering, [74:35] you're blazing new trails toward [74:38] understanding and reinvention. [74:43] You brought the physical world into the [74:46] virtual [74:48] to plan and optimize the world's modern [74:51] factories. You're building the next [74:54] frontier [74:56] where everything that moves is robotic. [75:00] Every car, an intelligent autonomous [75:04] agent, [75:07] and a new collaborative workforce to [75:10] help close the global labor shortage [75:12] gap. [75:15] Developers across the continent are [75:17] building every type of robot, [75:20] teaching them new skills [75:23] in digital twin worlds, and robot shows. [75:29] Preparing them to work alongside us in [75:32] our factories, [75:35] warehouses, [75:38] the operating room, [75:40] and at home. [75:42] The fourth industrial revolution is [75:45] here. Right where the first began. [75:52] What do you think? [76:00] I love that video. You made it. That's [76:03] so great. You made it. Well, we're um [76:07] working on industrial AI with one [76:09] company after another. This is uh BMW [76:13] doing building their next generation [76:14] factory in Omniverse. [76:19] This is uh [76:21] I don't know how to say it. Can somebody [76:23] teach me? [76:26] Sounds good. Um exactly. That's exactly [76:30] right. Good job. Good job. That's [76:32] exactly right. Uh they're they're [76:34] building of course their plants uh [76:36] digital twins and omniverse. This is [76:38] Keon, their uh uh uh their digital twin [76:42] for um uh warehouse logistics. This is [76:46] uh Mercedes-Benz and their digital twins [76:49] of their factories built in Omniverse. [76:51] This is Schaefer and their digital twin [76:54] of their warehouse built in Omniverse. [76:59] This is your train station here in [77:02] France building a digital twin of their [77:05] train stations in Omniverse. And this is [77:08] Toyota building a digital twin of their [77:11] warehouse in Omniverse. And when when [77:13] you build these warehouses and these [77:15] factories in Omniverse, then you could [77:17] you could you could you could design it, [77:19] you can plan it, you can change it in [77:22] green field is wonderful. In brown field [77:24] is wonderful. You could simulate its [77:26] effectiveness before you go and [77:27] physically lift and move things around [77:29] to discover it wasn't optimal. And so [77:32] the ability to do everything digitally [77:34] in a digital twin is incredible. But [77:38] the question is why does the digital [77:39] twin has to look photoreal and why does [77:42] it has to obey the laws of physics? The [77:45] reason for that is because we wanted [77:47] ultimately to be a digital twin where a [77:50] robot could learn how to operate as a [77:52] robot and robots rely on photons for [77:57] their perception system. And those [77:58] photons are generated through omniverse [78:02] and robots needs to interact with the [78:04] physical world so that it could know [78:07] whether it's doing the right things and [78:08] do learn how to do it properly. And so [78:10] these digital twins have to look real [78:13] and behave realistically. Okay, so [78:16] that's the reason why Omniverse was [78:17] built. This is uh this is fantastic. [78:20] This is a fusion reactor digital twin. [78:22] Incredibly complicated piece of [78:24] instrument as you know and without AI [78:26] the next generation fusion reactor would [78:28] not be possible. Well, we're we're [78:30] announcing today that we are going to [78:33] build [78:34] the world's first industrial AI cloud [78:38] here in Europe. I'm going to announce [78:40] Yep. [78:46] These industrial AI clouds are yes a lot [78:50] whole lot of computers in the cloud. [78:51] However, its requirement, its [78:53] performance, its safety requirement is [78:56] fundamentally different. And so I'm [78:58] going to tell you a lot more about it on [79:00] Friday. I'm only teasing you part of the [79:03] story today. But this industrial cloud [79:05] will be used for design and simulation. [79:09] Virtual wind tunnels that you just walk [79:11] into. [79:13] Virtual wind tunnels. You just move a [79:14] car into and you see it behave. Open [79:17] doors, open windows, change the design. [79:20] All completely in real time. Design in [79:23] real time. Simulate in a digital wind [79:25] tunnel. Digital twin of a wind tunnel in [79:27] real time. Build it in a factory of a [79:29] digital factory. Digital twin in real [79:32] time. all of this and let robots learn [79:35] how to be great robots [79:38] and build the robots of our future, [79:41] self-driving cars and such. We already [79:44] have tremendous ecosystem here. We've [79:46] been here as you know for a very long [79:48] time. Nvidia is 33 years old. The first [79:50] time we came to Europe was during the [79:52] time when workstations and the [79:54] digitalization of products CAD the CAD [79:57] revolution started. We were here during [80:00] the CE revolution and now the digital [80:02] twin revolution some two trillion [80:05] dollars of ecosystem here in Europe that [80:07] we part partner with and that we have [80:09] the privilege of supporting what comes [80:12] out of that is a new revolution that's [80:14] happening as you know everything that [80:17] moves will be robotics everything that [80:20] moves will be AIdriven and the car is [80:23] the most obvious next one [80:26] builds the AI supercomput computers to [80:29] train the model. The AI supercomputer [80:32] for Omniverse digital twins. We also [80:35] build the AI supercomputers for the [80:37] robots itself. [80:39] In every single case, whether it's in [80:42] the cloud for Omniverse or in the car, [80:45] we offer the entire stack. The computer [80:47] itself, the operating system that runs [80:50] on top of this computer, which is [80:51] different in every single case. this [80:54] computer high-speed sensor rich must be [80:58] functional safe in no in no circumstance [81:02] could it fail completely and so the [81:05] safety requirements incredibly high and [81:08] now we have an incredible model that [81:09] sits on top of it. This model that sits [81:12] on top of it is a transformer model. [81:14] It's a reasoning model and it takes [81:18] sensor in you tell it what you want it [81:20] to do and it will drive you there. [81:23] takes pixels in and it generates path [81:27] plans output. So it's a generative AI [81:30] model based on transformers. Incredible [81:33] technology. [81:34] NVIDIA's AI team, AV team is incredible. [81:38] This is the only team that I know that [81:40] has won endtoend self-driving car [81:43] challenge at CVPR two years in a row. [81:45] And so they're the winner again this [81:47] year. Let's take a look at the video. [81:48] Yep. Thank you. [81:54] Like any driver, autonomous vehicles [81:57] operate in a world full of unpredictable [81:59] and potentially safety critical [82:01] scenarios. [82:04] NVIDIA drive built on the Halo safety [82:07] system lets developers build safe [82:09] autonomous vehicles with diverse [82:12] software stacks and sensors and [82:14] redundant computers. [82:17] It starts with training. Safe AVs need [82:20] massive amounts of diverse data to be [82:23] able to address edge cases, but real [82:26] world data is limited. Developers use [82:29] NVIDIA Omniverse and Cosmos to [82:31] reconstruct the real world and generate [82:34] realistic synthetic training data to [82:36] bring diversity to the AV model. [82:42] The model can perceive and reason about [82:44] its environment, [82:46] predict future outcomes, and generate a [82:49] motion plan. [82:53] And for decision-making diversity, an [82:56] independent classical stack runs in [82:58] parallel. Guardrails monitor safe [83:01] performance [83:02] and in cases of anomalies calls the [83:05] arbitrator to make an emergency stop. [83:11] Further diversity and redundancy [83:14] are built into the sensor and compute [83:16] architecture. [83:19] Each sensor connects to redundant [83:21] computers. So even if a sensor or [83:24] computer fails, the vehicle stays safe [83:27] and operational. [83:30] And in the event of a critical failure, [83:32] the system can execute a minimum risk [83:34] maneuver like pulling over to the [83:36] shoulder. [83:38] Safety is foundational to autonomous [83:41] driving. NVIDIA Drive lets developers [83:44] worldwide integrate halos into their own [83:48] products to build the next generation of [83:50] safe AVs. [84:01] A billion cars on the road. 10,000 miles [84:05] a year on average. 10 trillion miles. [84:09] The future of autonomous driving is [84:11] obviously gigantic and it's going to be [84:14] driven is going to be powered by AI. [84:17] This is this is the next gigantic [84:20] opportunity and we're working with [84:23] enormous companies and really fantastic [84:25] companies around the world to make this [84:26] possible. At the core of everything we [84:29] do here with AV is safety and we're [84:31] really really proud of our Halo system. [84:35] It starts with the architecture of the [84:36] chip and then the chip design and the [84:38] systems design, the operating system, [84:41] the AI models, the methodology of [84:43] developing the software, the way we test [84:46] it. Everything from the way we train the [84:49] models, the data we provide for the [84:51] models all the way to the way we [84:53] evaluate the models. Nvidia's Halo [84:55] system and our AV safety team and [84:58] capabilities are absolutely [85:00] worldrenowned. This computer was the [85:03] first one to be softwaredefined. The [85:05] world's first softwaredefined completely [85:08] 100% softwaredefined AIdriven software [85:11] AIdriven stack for AVs. We've been at [85:14] this now for coming up on 10 years. And [85:16] so this capability is worldrenowned and [85:19] I'm really proud of it. [85:21] The same thing that's happening for cars [85:22] is happening for a new industry. As I [85:25] mentioned earlier, if you can generate [85:28] video from prompts, if AI can perceive, [85:32] it can reason, and it can generate [85:35] videos and words and images, and just [85:37] now with cars, the path, the steering [85:40] wheel path. Why can't it also generate [85:44] local motion abilities and articulation [85:46] abilities? So that fundamental ability [85:50] for AI to revolutionize one of the [85:53] hardest robotics problems is around the [85:55] corner. Humano or robots are going to be [85:58] a thing. We now know how to build these [86:01] things, train these things, and operate [86:03] these things. Human or robotics is going [86:06] to potentially be one of the largest [86:09] industries ever. And it requires [86:11] companies who know how to manufacture [86:13] things. manufacture things of [86:16] extraordinary capabilities. This speaks [86:20] of the European countries. So much of [86:22] the world's industries are based here. I [86:25] think this is going to be a giant [86:26] opportunity. Well, [86:29] let's say it's a billion robots around [86:31] the world. The idea that there would be [86:33] a billion robots is a very sensible [86:35] thing. Now, why hasn't it happened? [86:38] Well, the reason for that is simple. [86:40] Today's robots are too hard to program. [86:44] Only the largest companies can afford to [86:47] install a robot, get it to teach it, [86:49] program it to do exactly the right [86:51] things. Keep it sufficiently surrounded [86:54] so that it's safe. That's the reason why [86:57] the world's largest car companies all [86:59] have robots. They're large enough, the [87:02] work is sufficiently repetitive. It is [87:05] the industry is at a sufficient scale [87:07] that you could deploy robots into those [87:10] factories. Almost everybody who's a [87:13] middle or small medium medium companies [87:16] or mom and pop restaurants or stores or [87:19] warehouses, it's impossible to have that [87:23] programming capability. Until now, we're [87:26] going to give you essentially robots [87:28] where you could teach them. They'll [87:30] learn from you. Just as we were talking [87:33] about agentic AI, we now have humanoid [87:36] AI that can learn from your teaching [87:39] using toolkits that are very s very [87:41] consistent with the Nemo toolkits I I [87:43] spoke about. Nvidia here as well is [87:46] built in three layer stack. We build the [87:48] computer the Thor the Thor computer [87:51] devkit looks a little bit like this. [87:56] This is a robotic computer completely [87:59] self-contained dev kit sits on your [88:02] desk. These are all the sensors and [88:05] inside is a little supercomput Thor [88:08] chip. Just really really incredible. And [88:10] these Yep. [88:17] I could I could imagine getting one of [88:19] these inserted like that. [88:23] Okay. Thank you, Janine. And so that's [88:26] the Thor processor. On top is an [88:28] operating system designed for robotics. [88:30] And on top of that, transformer models [88:34] that take sensor and instructions and [88:38] transforms it and generates [88:41] flight or paths and motor controls for [88:46] arm articulation, finger articulation, [88:48] and of course your legs articulation. [88:51] Now the big challenge of human robotics [88:55] is the amount of data necessary to train [88:57] it is very very hard to get. And so the [89:00] question is how do you do that? Well the [89:02] way you solve that problem is to back in [89:04] omniverse a digital twin world that [89:07] obeys the laws of physics. And this is [89:10] an incredible piece of work that we're [89:11] doing. [89:14] Don't do it. Don't [89:21] Oh, my my fault. [89:24] Okay, these are robots. [89:27] [Applause] [89:32] We have [89:34] we develop computers to simulate to [89:36] train them, computers to simulate them, [89:38] and the computer that goes inside them. [89:40] There's a whole bunch of human robotics [89:42] companies being built around the world. [89:44] They all see the great opportunity to [89:47] revolutionize this new new device, if [89:50] you will. And uh the progress is going [89:52] incredibly fast. And the way that they [89:54] all learn is they learn in a virtual [89:57] world. And this virtual world has to [89:59] obey the laws of physics. And recently, [90:01] we announced a big partnership with [90:04] Disney Research and Deep Mind. And we're [90:07] going to work together to create the [90:09] world's most sophisticated physics [90:11] simulation. And I'm just trying to [90:13] figure out at this point how to go to [90:16] that slide. [90:17] Teach me who's with me. [90:23] This is what happens when you only [90:25] rehearse once. [90:28] Okay. So, this this [90:32] incredible system [90:35] this incredible system is where an AI [90:39] learns how to be an AI. Let me show it [90:41] to you. [90:43] [Music] [90:46] What's up? [90:47] [Music] [91:24] We have a special guest. [91:26] [Applause] [91:33] Your name is Gre. [91:35] Are are you are you a petite Garson or [91:40] petite Bill? [91:42] Okay. He's Greck is a little girl. [91:49] Now look at this. Gre learned how to [91:52] walk inside Omniverse [91:55] obeying the laws of physics. And by [91:58] inside Omniverse we created hundreds of [92:00] thousands of scenarios. [92:02] Then finally when Greck learned how to [92:05] operate and walk and manipulate in those [92:09] environments on sand and on you know on [92:12] gravel on slippery floors on concrete on [92:15] carpet [92:17] then when it comes when Greg comes into [92:19] the physical world the physical world is [92:22] just 100,0001 version of the world and [92:25] so you learn how to walk in the virtual [92:28] world and look at you now. [92:32] Can you can you jump? [92:38] Wow. [92:44] Can you dance? [92:54] Well, I think I think um I just want to [92:57] let you know I am the keynote presenter. [93:01] So, I need you I need you to behave. I [93:03] need you to behave for a few seconds. [93:06] I need you to behave for a few Could you [93:07] sit? Sit. [93:11] Hey, you know what we should do? Let's [93:13] take a picture of everybody. [93:17] [Music] [93:18] Yeah. [93:19] Bam. Bam. [93:28] Would you like to come home with me? [93:29] Would you like to come home with me? I [93:31] got Yeah, I know. [93:34] Yeah, I have pets. They would like to [93:36] have you as a pet. [93:39] No. No. [93:43] You're so smart. You're so smart. Well, [93:47] incredible, right? [93:49] [Applause] [93:56] You are the world's best robot. And [93:58] someday we'll all have one like you and [94:00] they'll follow us around. [94:06] But if I need if I need a glass of [94:08] whiskey, you're going to have to go tell [94:10] somebody else to go get me a glass of [94:11] whiskey because you have no arms. [94:16] Yeah. You're so cute. Okay, little girl. [94:21] You stay here for a second. Let's wrap [94:22] up. [94:27] All right. [94:30] It's very clear. It's very clear an [94:33] industrial revolution has started. The [94:36] next the next waves of AI has started. [94:40] Greck is a perfect example of what's [94:42] possible now with robotics. the [94:45] technology necessary to teach a robot, [94:48] to manipulate, to simulate, and of of [94:52] course the manifestation of an [94:55] incredible robot is now right in front [94:58] of us. We have physical robots and we [95:01] have information robots. We call them [95:03] agents. So, the next wave of AI has [95:05] started. It's going to require inference [95:09] workloads to explode. It's basically [95:11] going to go exponential. The number of [95:14] people that are using inference has gone [95:16] from 8 million to 800 million a 100 [95:18] times in just a couple of years. The [95:20] number the amount of prompts that the [95:22] tokens generate as I mentioned earlier [95:25] from a few hundred tokens to thousands [95:27] of tokens. And of course we use AI even [95:30] more than than ever today. So we need a [95:32] special computer designed for thinking [95:34] designed for reasoning and that's what [95:36] blackwell is a thinking machine. These [95:39] black wells will go into new types of [95:41] data centers, essentially AI factories [95:43] designed for one thing and one thing [95:45] only. And these AI factories are going [95:47] to generate tokens. And these tokens are [95:49] going to become your food, little Greck. [95:52] Yeah, I know. I know. And what's really, [95:56] really incredible, I'm so happy to see [95:58] that Europe is going allin on AI. The [96:01] amount of AI infrastructure being built [96:03] here will increase by an order of [96:05] magnitude [96:07] in the next couple years. I want to [96:09] thank all of you for your partnership. [96:10] Have a great Vivate. [96:12] Thank you. [96:16] Say bye-bye. Say bye-bye. Take a bunch [96:19] of pictures. Take a bunch of pictures. [96:22] Take a bunch of pictures. [96:25] Yeah. [96:27] [Applause] [96:33] [Music] [96:46] [Music] 